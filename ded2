import pandas as pd
import numpy as np
import umap
import hdbscan
import faiss
from tqdm import tqdm
from sentence_transformers import SentenceTransformer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.cluster import KMeans

class HighAccuracyHDBSCANClustering:
    def __init__(self, input_df, text_column="short_description"):
        self.df = input_df.copy()
        self.text_column = text_column
        self.model = SentenceTransformer("all-MiniLM-L6-v2")  # Keeping the original model

        self.umap_model = umap.UMAP(
            n_neighbors=50,  # Higher neighbors for better accuracy
            n_components=50, 
            min_dist=0.0, 
            metric="cosine", 
            random_state=42,
            low_memory=True
        )

        self.clusterer = hdbscan.HDBSCAN(
            min_cluster_size=10,
            min_samples=1,
            metric="euclidean",
            cluster_selection_method="eom",
            prediction_data=True,
            core_dist_n_jobs=-1,
            probability=True  # Enables soft clustering
        )

        self.vectorizer = CountVectorizer(stop_words="english", ngram_range=(1, 2), min_df=1)
        self.kmeans = KMeans(n_clusters=50, n_init="auto")  # For refining unclustered points

    def encode_texts(self, texts, batch_size=512):
        embeddings = []
        for i in tqdm(range(0, len(texts), batch_size), desc="Encoding Texts"):
            batch = texts[i:i + batch_size]
            batch_embeddings = self.model.encode(batch, convert_to_numpy=True)
            embeddings.append(batch_embeddings)
        return np.vstack(embeddings)

    def fast_umap(self, embeddings):
        index = faiss.IndexFlatL2(embeddings.shape[1])
        index.add(embeddings)
        _, neighbors = index.search(embeddings, 50)  # More neighbors = better local structure
        return self.umap_model.fit_transform(embeddings, neighbors=neighbors)

    def fit(self):
        self.df[self.text_column] = self.df[self.text_column].fillna("")
        texts = self.df[self.text_column].tolist()

        embeddings = self.encode_texts(texts)
        reduced_embeddings = self.fast_umap(embeddings)

        hdbscan_labels = self.clusterer.fit_predict(reduced_embeddings)
        cluster_probs = self.clusterer.probabilities_

        outliers = np.where(hdbscan_labels == -1)[0]
        if len(outliers) > 0:
            outlier_embeddings = reduced_embeddings[outliers]
            outlier_labels = self.kmeans.fit_predict(outlier_embeddings)
            for i, idx in enumerate(outliers):
                hdbscan_labels[idx] = outlier_labels[i] + max(hdbscan_labels) + 1

        self.df["Cluster"] = hdbscan_labels
        self.df["Cluster Confidence"] = cluster_probs

        return self.df

df = pd.read_csv("incident_gaf1_100k.csv", encoding="latin1")
clustering_model = HighAccuracyHDBSCANClustering(df)
clustered_df = clustering_model.fit()
